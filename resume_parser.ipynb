{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "viS02IyFxUqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d96ca0f-3a9a-4fbe-e9ee-47b896dab3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.0-cp310-none-manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.0 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.0 pymupdf-1.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz"
      ],
      "metadata": {
        "id": "N0c831A3jSzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf_file:\n",
        "        for page_num in range(len(pdf_file)):\n",
        "            page = pdf_file.load_page(page_num)\n",
        "            text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "def convert_to_html(pdf_path, html_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    with open(html_path, \"w\", encoding=\"utf-8\") as html_file:\n",
        "        html_file.write(f\"<html><body><pre>{text}</pre></body></html>\")\n",
        "\n",
        "# Specify the path to your PDF and where you want to save the HTML file\n",
        "pdf_path = \"/content/PRAGANNISHA_K_.pdf\"\n",
        "html_path = \"/content/nithya.html\"\n",
        "\n",
        "convert_to_html(pdf_path, html_path)\n"
      ],
      "metadata": {
        "id": "m4RtkHBPZx-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf_file:\n",
        "        for page_num in range(len(pdf_file)):\n",
        "            page = pdf_file.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            for b in blocks:\n",
        "                if \"lines\" in b:\n",
        "                    for line in b[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text += span[\"text\"]\n",
        "    return text\n",
        "\n",
        "def convert_to_html(pdf_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    # Remove angle braces and irrelevant HTML tags\n",
        "    text = text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "    html_code = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Resume</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <pre>{text}</pre>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_code\n",
        "\n",
        "# Specify the path to your PDF\n",
        "pdf_path = \"/content/PRAGANNISHA_K_.pdf\"\n",
        "\n",
        "html_code = convert_to_html(pdf_path)\n",
        "print(html_code)\n"
      ],
      "metadata": {
        "id": "zYVwU5eWaBZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_and_font_sizes_from_pdf(pdf_path):\n",
        "    font_sizes = []\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf_file:\n",
        "        for page_num in range(len(pdf_file)):\n",
        "            page = pdf_file.load_page(page_num)\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            for b in blocks:\n",
        "                if \"lines\" in b:\n",
        "                    for line in b[\"lines\"]:\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text += span[\"text\"]\n",
        "                            font_size = span[\"size\"]\n",
        "                            font_sizes.append(font_size)\n",
        "    return text, font_sizes\n",
        "\n",
        "def convert_to_html(pdf_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    # Remove angle braces and irrelevant HTML tags\n",
        "    text = text.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "    html_code = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Resume</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <pre>{text}</pre>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_code\n",
        "\n",
        "def calculate_average_font_size(pdf_path):\n",
        "    _, font_sizes = extract_text_and_font_sizes_from_pdf(pdf_path)\n",
        "    if len(font_sizes) == 0:\n",
        "        return None\n",
        "    average_font_size = sum(font_sizes) / len(font_sizes)\n",
        "    return average_font_size\n",
        "\n",
        "# Specify the path to your PDF\n",
        "pdf_path = \"/content/PRAGANNISHA_K_.pdf\"\n",
        "\n",
        "average_font_size = calculate_average_font_size(pdf_path)\n",
        "if average_font_size is not None:\n",
        "    print(f\"Average Font Size: {average_font_size}\")\n",
        "else:\n",
        "    print(\"No font sizes found in the PDF.\")\n"
      ],
      "metadata": {
        "id": "tFSdrPbOadyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_heading(line):\n",
        "    # Check if the line is in uppercase\n",
        "    return line.isupper()\n",
        "\n",
        "def extract_segments(text):\n",
        "    segment_queue = []\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_segment = \"\"\n",
        "    for line in lines:\n",
        "        if is_heading(line):\n",
        "            if current_segment:\n",
        "                segment_queue.append(current_segment.strip())\n",
        "                current_segment = \"\"\n",
        "        current_segment += line + \"\\n\"\n",
        "    # Append the last segment\n",
        "    if current_segment:\n",
        "        segment_queue.append(current_segment.strip())\n",
        "    return segment_queue\n",
        "\n",
        "# Specify the path to your PDF\n",
        "pdf_path = \"/content/PRAGANNISHA_K_.pdf\"\n",
        "\n",
        "# Extract text from the PDF\n",
        "text, _ = extract_text_and_font_sizes_from_pdf(pdf_path)\n",
        "\n",
        "# Initialize the segment queue\n",
        "segment_queue = extract_segments(text)\n",
        "\n",
        "print(\"Segment Queue:\")\n",
        "for i, segment in enumerate(segment_queue, 1):\n",
        "    print(f\"Segment {i}: {segment}\")\n"
      ],
      "metadata": {
        "id": "ohDWk87ra_Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "working from here:"
      ],
      "metadata": {
        "id": "bR9EW2oXiY5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "case insesnsitive"
      ],
      "metadata": {
        "id": "SdEtFORboimn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKvtM7yNngTx",
        "outputId": "bf2d7430-673e-4ea0-da85-cfc04e6d6fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20231228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text"
      ],
      "metadata": {
        "id": "S52FcyCOnq1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf_file:\n",
        "        for page_num in range(len(pdf_file)):\n",
        "            page = pdf_file.load_page(page_num)\n",
        "            text += page.get_text(\"text\")\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove angle braces and irrelevant HTML tags\n",
        "    clean_text = re.sub(r'<[^>]*>', '', text)\n",
        "    clean_text = re.sub(r'[<>]', '', clean_text)\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def convert_to_html(pdf_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    cleaned_text = clean_text(text)\n",
        "    resume_text = cleaned_text.split('\\n')\n",
        "\n",
        "    html_code = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Resume</title>\n",
        "    </head>\n",
        "    <body>\n",
        "    \"\"\"\n",
        "    for line in resume_text:\n",
        "        html_code += f'<span style=\"{line.strip()}\">{line.strip()}</span><br/>\\n'\n",
        "\n",
        "    html_code += \"\"\"\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_code\n",
        "\n",
        "def extract_sections(html_path):\n",
        "    with open(html_path, \"r\", encoding=\"utf-8\") as html_file:\n",
        "        soup = BeautifulSoup(html_file, \"html.parser\")\n",
        "\n",
        "    sections = {}\n",
        "    current_section = None\n",
        "\n",
        "    section_titles = [\"Education\", \"Projects\", \"Skills\", \"Interests\", \"Paper Publications\"]\n",
        "\n",
        "    for span in soup.find_all(\"span\"):\n",
        "        text = span.text.strip()\n",
        "        for title in section_titles:\n",
        "            if title.lower() in text.lower():\n",
        "                current_section = title\n",
        "                sections[current_section] = []\n",
        "                break\n",
        "        else:\n",
        "            if current_section and text:\n",
        "                sections[current_section].append(text)\n",
        "\n",
        "    return sections\n",
        "\n",
        "\n",
        "# Specify the path to your PDF\n",
        "pdf_path = \"/content/PRAGANNISHA_K_.pdf\"\n",
        "\n",
        "html_code = convert_to_html(pdf_path)\n",
        "\n",
        "# Save the HTML code to a file\n",
        "with open(\"/content/praganniisha.html\", \"w\", encoding=\"utf-8\") as html_file:\n",
        "    html_file.write(html_code)\n",
        "\n",
        "# Extract sections from the HTML file\n",
        "sections = extract_sections(\"/content/praganniisha.html\")\n",
        "\n",
        "for section, content in sections.items():\n",
        "    print(section)\n",
        "    print(\"\\n\".join(content))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "x5_AAlAyhok5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4df292-bea5-49fb-b1da-df26dc2ab62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Education\n",
            "MSc Artificial Intelligence and Machine Learning\n",
            "October 2020  –  May 2025\n",
            "Coimbatore Institute of Technology\n",
            "8.44 CGPA (until 4th semester)\n",
            "HIGHER SECONDARY\n",
            "March 2020\n",
            "SBOA Matric And Higher Secondary School\n",
            "90% (average)\n",
            "\n",
            "\n",
            "Projects\n",
            "Prediction of suitable crops for a specific soil\n",
            "-Based on the type of soil, the optimal set of crops to be sowed is predicted. To classify the crops, Image processing, and\n",
            "Deep neural networks were applied to the dataset obtained from Agri University.\n",
            "-Language and Framework: Python, TensorFlow, OpenCV\n",
            "Generative Adversarial Networks\n",
            "-A basic GAN model was built over the fashion-mnist dataset which is loaded from The TensorFlow dataset.\n",
            "New synthetic datasets were generated and the discriminator has to distinguish between fake and original images.\n",
            "-The final deep learning model has been trained over 500 epochs in order to produce accurate images.\n",
            "Sign Language Detection\n",
            "-Developed a sign language detection model using a vgg-16 architecture with primary sourced data of people with hearing\n",
            "and speech impairment.\n",
            "-Language and Framework: Python, TensorFlow, and Heroku.\n",
            "-Deployed using streamlit\n",
            "-Got an accuracy of 90%\n",
            "PUBLICATIONS\n",
            "Identification of star constellations using Machine Learning:\n",
            "Pattern recognition algorithms are used to associate patterns with each star in the image to the star directory. Published it\n",
            "at the International Conference on Digital Information and Computing Technologies 2022.\n",
            "\n",
            "\n",
            "Skills\n",
            "Programming:\n",
            "C, Python, Database-Oracle\n",
            "Front-End: HTML, CSS\n",
            "\n",
            "\n",
            "Interests\n",
            "Machine Learning, Deep Learning, Database Management Systems, and Image Processing.\n",
            "Certifications:\n",
            "NPTEL Database management systems\n",
            "NPTEL Python for Data Science\n",
            "Complete Python Bootcamp (Udemy)\n",
            "Front-end Web Developer Bootcamp (Udemy)\n",
            "Extra Curricular:\n",
            "CIT Music Club   (2021-present)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing CSV file"
      ],
      "metadata": {
        "id": "iODfX6PZuOFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "from resumpr import ResumeParser\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store data\n",
        "resume_data = []\n",
        "\n",
        "# Iterate over each folder\n",
        "for folder_name in range(1, 5):\n",
        "    folder_path = f\"path/to/folder/{folder_name}\"\n",
        "    pdf_files = os.listdir(folder_path)\n",
        "\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            # Extract text from each page of the PDF\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "\n",
        "            # Use ResumeParser to extract skills from the extracted text\n",
        "            parser = ResumeParser()\n",
        "            skills = parser.extract_skills(text)\n",
        "\n",
        "        # Append the resume data to the list\n",
        "        resume_data.append({\n",
        "            'resume': text,\n",
        "            'skills': skills,\n",
        "            'job_role': job_roles[folder_name - 1],\n",
        "\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(resume_data)\n",
        "\n",
        "df.to_csv('resume_data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "xc4N-Gatl6Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec and RNN"
      ],
      "metadata": {
        "id": "XdaD62OCreWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Load data from CSV\n",
        "data = pd.read_csv('resume_data.csv')\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=data['skills'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['skills'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text data to sequences\n",
        "sequences = tokenizer.texts_to_sequences(data['skills'])\n",
        "max_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Create input data by concatenating the padded sequences\n",
        "X = np.array([word2vec_model.wv[tokenizer.index_word[idx]] for idx in padded_sequences.flatten()]).reshape(len(sequences), max_length, 100)\n",
        "\n",
        "# Create the RNN model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "\n",
        "y = np.random.rand(len(data))\n",
        "model.fit(X, y, epochs=10, batch_size=1)\n",
        "\n",
        "# Use the trained model to predict scores for the data\n",
        "scores = model.predict(X).flatten()\n",
        "\n",
        "data['score'] = scores\n",
        "\n",
        "data.to_csv('resume_data_with_scores.csv', index=False)\n"
      ],
      "metadata": {
        "id": "PgvrwWiyquoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics"
      ],
      "metadata": {
        "id": "dkf5e8SsuTrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "y_pred = model.predict(X).flatten()\n",
        "\n",
        "\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R-squared (R2) Score: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIAIglcBpqzh",
        "outputId": "a5bd1613-055d-4ab4-b0a6-54e949709fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.025\n",
            "R-squared (R2) Score: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ewDbVugpxNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}